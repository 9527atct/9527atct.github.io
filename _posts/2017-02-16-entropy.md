---
layout: post
title: entropy and kl divergence
date: 2017-02-16 21:45
categories: MachineLeanring
comments: true
---


### Entropy ###
+ [**definition**]  
    Given discrete random variable $X$, the entropy $H(X)$ of $X$ is defined by,
    \\[
        H(X)=-\\sum_x p(x)\\log p(x)
    \\]

+ [**non negative**]  
    For any discrete random variable $X$, we can conclude that $H(X)\\geq 0$.

+ [**example**]  
    R.V. $X$, $p(X=0)=1-r, p(X=1)=r, (0\\leq r\\leq 1)$, then, the entropy of $X$ is,
    \\[
        H(X)=-r\\log r -(1-r)\\log(1-r)
    \\]

+ [**joint entropy**]  
\\[ 
    H(X,Y)=-\\sum_x \\sum_y p(x,y) \\log(p(x,y)) = -E\_{p(x,y)}[\\log(x,y)]
\\]

+ [**conditional entropy**]
\\[
\\begin{split}
H(Y|X) &= \\sum_x p(x) H(Y|X=x) \\\
&= -\\sum_x p(x) \\sum_y p(y|x)\\log p(y|x)\\\
&= -\\sum_x\\sum_y p(x,y)\\log p(y|x)\\\
&= -E\_{p(x,y)}[\\log(y|x)]
\\end{split}
\\]

+ [**chain rule**]  
    \\[
        H(X,Y)=H(X)+H(Y|X)
    \\]


### Relative Entropy and Mutual Information ###
+ [**relative entropy (KL divergence)**]  
    Relative Entropy or KL divergence between pdf $p(x)$ and $q(x)$ is
    \\[
        KL(p||q)=\\sum_x p(x)\\log\\left(\\frac{p(x)}{q(x)}\\right)=E\_{p}\\left(\\log\\left( \\frac{p(x)}{q(x)} \\right)\\right)
    \\]
    Generally, the KL divergence is not symmetric, but we can build  $KL(q||q)=1/2 KL(p||q)+1/2 KL(q||p)$ to make KL divergence symmetric.

+ [**example**]  
    Let $\\mathcal{X}=\\{0,1\\}$, $p(x),q(x)$ are pdf(pmf), and, $p(X=0)=1-r$, $p(X=1)=r$, $q(X=0)=1-s$, $q(X=1)=s$. Then,
    \\[
    \\begin{split}
    KL(p||q) &= (1-r)\\log\\left( \\frac{1-r}{1-s} \\right) + r\\log\\left(\\frac{r}{s} \\right) \\\
    KL(q||p) &= s \\log\\left(\\frac{s}{r}\\right) +(1-s)\\log\\left( \\frac{1-s}{1-r} \\right)
    \\end{split}
    \\]

+ [**mutual information**]  
    Given two variable $X,Y$ with pdf(pmf) $p(x,y)$ and marginal $p(x),p(y)$, then the mutual information $I(X,Y)$ is
    \\[
        I(X,Y) = \\sum_x\\sum_y p(x,y)\\log\\left( \\frac{p(x,y)}{p(x)p(y)}\\right)=KL(p(x,y)||p(x)p(y))
    \\]

+ [**theorem**]
    \\[
    \\begin{split}
    I(X,Y)&=I(Y,X)\\\
    I(X,X)&=H(X)\\\
    I(X,Y)&=H(X)-H(X|Y) \\\
    &= H(Y)-H(Y|X)\\\
    &=H(X)+H(Y)-H(X,Y)
    \\end{split}
    \\]

+ [**conditional mutual information**]  
    The conditional mutual information of random variable $X$ and $Y$ given $Z$ is
    \\[
        I(X,Y|Z)=H(X|Z)-H(X|Y,Z)
    \\]

+ [**theorem**]  
    Let $p(x)$ and $q(x)$ with $x\\in \\mathcal{X}$ be two pdf(pmf). Then $KL(p||q)\\geq 0$ with the equality if and only if $p(x)=q(x)$, for all $x\\in \\mathcal{X}$.


### Differential Entropy###
\\[
H(X)=-\\int\_\\mathcal{S} f(x)\\log f(x) dx
\\]
where $\\mathcal{S}$ is the support set of random variable $X$, and $f(x)$ is pdf of $X$.

- [**conditional differential entropy**]  
    \\[
        H(X|Y)=-\\int f(x,y)\\log(p(x|y))
    \\]

- [**KL divergence**]  
    Suppose $X\\sim f(X)$ and $Y\\sim g(X)$, then
    \\[
        KL(f||g)=\\int f(x)\\log \\left( \\frac{f(x)}{g(x}\\right)
    \\]
    Note that $KL(f||q)$ is finite only if the support of $f$ is contained in the support of $g$

- [**mutual information**]  
    \\[
        I(X,Y)=\\int f(x,y)\\log \\left( \\frac{f(x,y)}{f(x)f(y)} \\right) = KL(f(x,y)||f(x)f(y))
    \\]
    \\[
        I(X,Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)
    \\]

- [**theorem**]  
    \\[
        KL(f||q) \\geq 0
    \\]
    with the equality iff $f=g$ at almost everywhere.

- [**corollary**]  
    - $I(X,Y)\\geq 0$, with the equality iff $X$ and $Y$ are independent.
    - $H(X|Y)\\leq H(X)$, with the equality iff $X$ and $Y$ are independent.

- [**chain rule for differential entropy**]  
    \\[
        H(X\_1,...,X\_n)=\\sum\_{i=1}^n H(X\_i|X\_1,...,X\_{i-1})
    \\]

- [**corollary**]  
    \\[
        H(X\_1,...,X\_n) \\leq \\sum\_{i=1}^n H(X\_i
        )
    \\]

- [**theorem**]  
    \\[
        H(aX+c)=H(X)+\\log |a|
    \\]
    where $a\\geq 0$.  
    *proof prompt.* using the transformation $Y=aX+c$. 

- [**corollary**]  
    Suppose $A$ is nonsingular, then $H(AX)=H(X)+\\log |A|$.

- [**theorem**]  
    Let $X\\in R^m$ have zero mean and covariance $\\Sigma=[XX^T]$, then
    \\[
        H(X)\\leq \\frac{1}{2}\\log((2\\pi)^n |\\Sigma|)+\\frac{n}{2}
    \\]  
    *proof prompt*: $KL(q||f)=-H(X)-\\int g\\log f\\geq 0$

### Exponential Family and the optimization solution of the best pdf with some constraints###

Consider the pdf $p(x)$ which satisfies the $k$ (independent) constraints,
\\[
\\int\_{\\mathcal{X}} h\_i(x)p(x)dx=m\_i \\leq \\infty, i=1,...,k
\\]
where $m_i$ is specified constrants. We want to find certain pdf $p(x)$ that is closet to $f(x)$. That is,
\\[
\\begin{split}
min\_p &\\int p(x)\\log\\frac{p(x)}{f(x)} \\\
s.t. &\\int_\\mathcal{X} h\_i(x)p(x)dx=m\_i,\\quad i=1,...,k\\\
&\\int p(x)dx=1
\\end{split}
\\]

using Lagrangian multiply methods, we obtain the object function,
\\[
F(p)=\\int p(x)\\log\\frac{p(x)}{f(x)} +\\sum\_i \\theta\_i \\left(\\int_\\mathcal{X} h\_i(x)p(x)dx-m\_i\\right)+c\\left( \\int\_\\mathcal{X} p(x)dx -1 \\right)
\\]

calculate the Functional derivative, using the following formula,
\\[
    \\int \\frac{\\delta F }{\\delta p}\\phi(x)dx=\\lim_{\\epsilon\\rightarrow 0} \\frac{F(p+\\epsilon \phi) -F(p)}{\\epsilon}
\\]
where $\\phi(x)$ is an arbitary function of the $x$.

Then, we have,
\\[
0=\\int \\frac{\\delta F }{\\delta p}\\phi(x)dx = \\lim_{\\epsilon\\rightarrow 0} (\\int p(x)\\log(1+\\epsilon \\frac{\phi(x)}{p(x)})dx +\\epsilon\\sum\_{i=1}^k \\int \\theta\_i h\_i(x)\\phi(x)dx+\\epsilon c\\int \\phi(x)dx)
\\]

Thus,
\\[
c+1+\\log\\frac{p(x)}{f(x)}+\\sum\_{i=1}^k\\theta\_ih\_i(x)=0
\\]
which means $p(x)=\\frac{1}{g(\\theta)}f(x)\\exp(\\sum\_{i=1}^k\\theta\_ih\_i(x))$, where $g(\\theta)=\\int\_{x\\in \\mathcal{X}}f(x)\\exp(\\sum\_{i=1}^k\\theta\_ih\_i(x))$.


